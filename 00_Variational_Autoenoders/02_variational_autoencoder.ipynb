{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transform: Normalize images to [-1,1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.5,), (0.5,)),\n",
    "    #transforms.ConvertImageDtype(torch.float)\n",
    "])\n",
    "\n",
    "# Load Fashion-MNIST dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root=\"./data\", train=True, transform=transform)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VAE Model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # -> [B, 32, 14, 14]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # -> [B, 32, 7, 7]\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # -> [B, 64, 4, 4]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)  # -> [B, 64, 2, 2]\n",
    "        )\n",
    "\n",
    "        self.fc1  = nn.Linear(64 * 2 * 2, 400)  # match encoder output\n",
    "        self.fc21 = nn.Linear(400, 20)  # mu\n",
    "        self.fc22 = nn.Linear(400, 20)  # logvar\n",
    "\n",
    "        # Latent to decoder\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 64 * 7 * 7)\n",
    "\n",
    "        # Decoder (transposed convs)\n",
    "        self.deconv1 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)  # [B, 64, 2, 2]\n",
    "        x = x.view(x.size(0), -1)  # flatten to [B, 256]\n",
    "        h = F.relu(self.fc1(x))   # [B, 400]\n",
    "        return self.fc21(h), self.fc22(h)  # mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x = F.relu(self.fc3(z))             # [B, 400]\n",
    "        x = F.relu(self.fc4(x))             # [B, 3136]\n",
    "        x = x.view(-1, 64, 7, 7)            # reshape to [B, 64, 7, 7]\n",
    "        x = F.relu(self.deconv1(x))         # -> [B, 32, 14, 14]\n",
    "        x = torch.sigmoid(self.deconv2(x))  # -> [B, 1, 28, 28]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "\n",
    "\n",
    "'''\n",
    "# Loss function: Reconstruction loss + KL Divergence loss\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.F.binary_cross_entropy(recon_x.view(x.size(0), -1), x.view(x.size(0), -1), reduction='sum')\n",
    "    KL = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KL\n",
    "'''\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    recon_x_flat = recon_x.view(x.size(0), -1)\n",
    "    x_flat = x.view(x.size(0), -1)\n",
    "\n",
    "    MSE = F.mse_loss(recon_x_flat, x_flat, reduction='mean')\n",
    "    KL = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())/x.size(0)\n",
    "    return (MSE + KL) #/ x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 1.5245\n",
      "Epoch [2/30], Loss: 1.2912\n",
      "Epoch [3/30], Loss: 1.2820\n",
      "Epoch [4/30], Loss: 1.2791\n",
      "Epoch [5/30], Loss: 1.2782\n",
      "Epoch [6/30], Loss: 1.2775\n",
      "Epoch [7/30], Loss: 1.2769\n",
      "Epoch [8/30], Loss: 1.2765\n",
      "Epoch [9/30], Loss: 1.2764\n",
      "Epoch [10/30], Loss: 1.2762\n",
      "Epoch [11/30], Loss: 1.2759\n",
      "Epoch [12/30], Loss: 1.2756\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss, optimizer\n",
    "model = VAE().to(device)   # default to model.train() if model.eval() is not called before training\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for images, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        recon_images, mu, logvar = model(images)\n",
    "        loss = loss_function(recon_images, images, mu, logvar)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize reconstructed images\n",
    "def visualize_reconstruction(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        images, _ = next(iter(data_loader))\n",
    "        images = images.to(device)\n",
    "        recon_images, _, _ = model(images)\n",
    "\n",
    "        images = images.cpu().numpy()\n",
    "        recon_images = recon_images.cpu().numpy()\n",
    "\n",
    "        fig, axes = plt.subplots(2, 10, figsize=(10, 2))\n",
    "        for i in range(10):\n",
    "            axes[0, i].imshow(images[i][0] * 0.5 + 0.5, cmap='gray')  # Original\n",
    "            axes[0, i].axis('off')\n",
    "            axes[1, i].imshow(recon_images[i][0] * 0.5 + 0.5, cmap='gray')  # Reconstructed\n",
    "            axes[1, i].axis('off')\n",
    "\n",
    "        axes[0, 0].set_title(\"Original\")\n",
    "        axes[1, 0].set_title(\"Reconstructed\")\n",
    "        plt.show()\n",
    "\n",
    "# Run visualization\n",
    "visualize_reconstruction(model, train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
